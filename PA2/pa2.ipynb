{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Programming Assignment 2: Triage\n",
    "\n",
    "Now that you're familiar with Jupyter notebooks and have had some experience\n",
    "working with text data, it's time to begin investigating some real NLP tasks.\n",
    "In this assignment, you'll be focusing specifically on text classification,\n",
    "and we'll investigate two commonly used models for\n",
    "text classification tasks.\n",
    "\n",
    "Text classification tasks come up in a huge range of contexts; quite often we\n",
    "may be provided with text data in some form and be interested in labeling or\n",
    "categorizing it in some way. In this assignment, we'll be looking at one such\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task: Message Classification for Disaster Aid\n",
    "\n",
    "Victims of natural disasters have urgent needs for food, water, shelter,\n",
    "medicine, and other forms of aid.  These needs are often communicated through\n",
    "text messages, social media posts, and local newspapers. Because of their\n",
    "ability to automatically process large amounts of text, NLP techniques can\n",
    "play an important role in ensuring that people receive potentially life-saving\n",
    "aid.\n",
    "\n",
    "Our goal with this data will be to perform text classification on messages sent\n",
    "in the aftermath of natural disasters. Specifically, you will need to determine\n",
    "whether a specific message is about aid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Investigation\n",
    "\n",
    "As usual, the first thing to do is to understand and characterize the data!\n",
    "\n",
    "The data for this assignment contains about 26K documents from several major\n",
    " natural disasters:\n",
    "\n",
    "* [Earthquake in Haiti (2010)](https://en.wikipedia.org/wiki/2010_Haiti_earthquake)\n",
    "* [Floods in Pakistan (2010)](https://en.wikipedia.org/wiki/2010_Pakistan_floods)\n",
    "* [Earthquake in Chile (2010)](https://en.wikipedia.org/wiki/2010_Chile_earthquake)\n",
    "* [Hurricane Sandy in North America (2012)](https://en.wikipedia.org/wiki/Hurricane_Sandy)\n",
    "\n",
    "These documents are either text messages, social media (Twitter) posts,\n",
    "or snippets from news articles. In addition to the specific events listed above,\n",
    "the dataset contains a number of news articles spanning dozens of different\n",
    "disasters. All messages have been translated and annotated by humans on the\n",
    "crowdsourcing platform CrowdFlower (now Figure-Eight). However, some of the\n",
    "translations are not perfect, and you may encounter some words in other\n",
    "languages. Unfortunately, NLP researchers often have to work with \"messy\"\n",
    "data. If you're curious about the crowdsourcing translation effort for messages\n",
    "from Haiti in particular, feel free to check out [this paper](https://nlp.stanford.edu/pubs/munro2010translation.pdf).\n",
    "\n",
    "Your task is to classify each document as being aid-related (class \"aid\") or\n",
    "not aid-related (class \"not\"). Messages that are aid-related include\n",
    "individuals' requests for food/water/shelter/etc. The aid class also includes\n",
    "news reports about dire situations and disaster relief efforts.\n",
    "\n",
    "Below are several examples of aid-related documents (belonging to class \"aid\"):\n",
    "\n",
    "* Hello Good Morning We live on 31 Delmas we are without water without food and\n",
    "what we had have finished Please do something for us!\n",
    "* I am sending this SMS from Layah district for my sister whose house has got\n",
    "destroyed in a flood. So, the problem she faces now is that she hasn't got any\n",
    "'Watan Card'or any financial aid from the government. She has 5 children too.\n",
    "* Redcross came to my house and gave my family food ... Guess were not getting\n",
    "power anytime soon . #sandy #RedCross\n",
    "* Relief officials have stressed the vital importance of bringing in clean\n",
    "drinking water and sanitation equipment to avoid deadly epidemics that in a\n",
    "worst case scenario could claim as many or more lives than the tsunami itself.\n",
    "\n",
    "Below are several examples of non-aid-related documents\n",
    "(belonging to class \"not\"):\n",
    "\n",
    "* A cold front is found over Cuba this morning. It could cross Haiti tomorrow.\n",
    "Isolated rain showers are expected over our region tonight.\n",
    "* Hurricane : A storm which New Yorkers use as an excuse to drink and eat junk\n",
    "food in their pajamas for 48 hours . #sandy\n",
    "* By secret ballot, the Council elected Pakistan, Bahrain and the Republic of\n",
    "Korea from the Asian States, while Iran and Saudi Arabia did not receive enough\n",
    "votes to qualify.\n",
    "\n",
    "The data is divided into a training set, development (validation) set, and\n",
    "test set. Recall that the training set is used to learn (compute the statistics\n",
    "for) your model. These statistics are then used to classify the documents in the\n",
    "development and test sets. For this assignment, you have access to the\n",
    "training set and the dev set. The test set is hidden, but your submission\n",
    "will be evaluated on it as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at some of the data. First, we'll import some packages and\n",
    "helper methods that we require (you can see the implementation of the helper\n",
    "methods in util.py).\n",
    "\n",
    "__NOTE: You should NOT import or use any other packages except the ones imported\n",
    "below and other packages in the Python standard library. This means you should\n",
    "not use spaCy, NLTK, gensim, or other functionality in scikit-learn besides\n",
    "CountVectorizer, even though those are provided in the conda\n",
    "environment we set up for you. If your solution uses any such extra dependencies,\n",
    "it will fail the autograder.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from util import load_data, Classifier, Example, evaluate, remove_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We've defined a Dataset class for you to store the loaded data, and a function\n",
    "load_data() to load it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'util.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data(\"./data/triage\")\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inside the loaded dataset, you can find the training and development sets,\n",
    "each of which is a list of Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.train contains 21046 examples\n",
      "<class 'util.Example'>\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset.train contains {} examples\".format(len(dataset.train)))\n",
    "print(type(dataset.train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When loading the data, load_data will automatically load each line as a separate\n",
    "example and give it the corresponding label. It will also take the line and\n",
    "segment it into a list of words.\n",
    "\n",
    "The label is an integer that can take only one of two values (1 for aid, 0 for\n",
    "not aid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example:\n",
      "Words: ['with', 'support', 'from', 'partners', 'a', 'water', 'filtration', 'unit', 'has', 'been', 'successfully', 'installed', 'in', 'the', 'msf', 'managed', 'water', 'system', 'bladder', 'tank', 'in', 'bawe', 'resettlement', 'centre', 'mutarara', 'district']\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"First training example:\")\n",
    "print(\"Words: {}\".format(dataset.train[0].words))\n",
    "print(\"Label: {}\".format(dataset.train[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, load_data() will also automatically shuffle your data for you, so the\n",
    "examples will be in a random order.\n",
    "\n",
    "It's not strictly necessary, but you may find it useful to know the following\n",
    "additional information about the data:\n",
    "\n",
    "- The training set is located in: ```data/train.csv```\n",
    "- The dev set is located in: ```data/dev.csv```\n",
    "\n",
    "Within each of these csv files, each line is a single example, consisting\n",
    "of a document (string) and a corresponding label.\n",
    "\n",
    "Note that the data you are given is already preprocessed;\n",
    "all punctuation has been removed (except hashtags and apostrophes) and all text\n",
    "has been converted to lowercase. Depending on the specific NLP task,\n",
    "preprocessing can significantly improve performance. You do not need to do any\n",
    "additional preprocessing for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## First Approach: Naive Bayes\n",
    "\n",
    "Now that we have our data set up, we can get started on implementing some\n",
    "classifiers! The first approach we'll try is Naive Bayes, which we discussed\n",
    "in the last few videos (if this doesn't ring a bell, definitely go back and\n",
    "revisit the lectures/slides on Naive Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    TODO: Implement the Multinomial Naive Bayes classifier with add-1 smoothing\n",
    "    (Laplace smoothing)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 filter_stop_words=False,\n",
    "                 use_bigrams=False):\n",
    "        super().__init__(filter_stop_words, use_bigrams)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def _bigram(self, word_seq): # self defined helper function\n",
    "        if not word_seq:\n",
    "            return []\n",
    "        bigram = [(word_seq[i], word_seq[i + 1]) for i in range(len(word_seq) - 1)]\n",
    "        bigram.extend([('<s>', word_seq[0]), (word_seq[len(word_seq) - 1], '</s>')])\n",
    "        return bigram\n",
    "\n",
    "    def train(self, examples: List[Example]) -> None:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function that takes in a list of labeled\n",
    "        examples and trains the classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_doc = len(examples) # total number of doc\n",
    "        doc_freq = np.array([0, 0]) # doc freq for each class\n",
    "        self.vocab = set() # vocabulary\n",
    "        counts = defaultdict(lambda : 0)\n",
    "        \n",
    "        for ex in examples:\n",
    "            doc_freq[ex.label] += 1 # update prior freq\n",
    "            ex_words = remove_stop_words(ex.words, self.stop_words) if self.filter_stop_words else ex.words\n",
    "            if self.use_bigrams:\n",
    "                ngram = self._bigram(ex_words)\n",
    "            else:\n",
    "                ngram = ex_words\n",
    "            self.vocab.update(ngram) # merge words into V\n",
    "            ngram_freq = np.unique(np.array(ngram), return_counts=True)\n",
    "            for ng, freq in list(zip(ngram_freq[0], ngram_freq[1])):\n",
    "                counts[(ng, ex.label)] += freq\n",
    "        \n",
    "        self.logprior = np.log(doc_freq/num_doc)\n",
    "        denom = np.array([0, 0])\n",
    "        denom[0] = np.sum([count for (word, cls), count in counts.items() if cls == 0]) + len(self.vocab)\n",
    "        denom[1] = np.sum([count for (word, cls), count in counts.items() if cls == 1]) + len(self.vocab)\n",
    "        logllh0 = {(w, 0): np.log((counts[(w, 0)] + 1)/denom[0]) for w in self.vocab}\n",
    "        logllh1 = {(w, 1): np.log((counts[(w, 1)] + 1)/denom[1]) for w in self.vocab}\n",
    "        self.logllh = {**logllh0, **logllh1}        \n",
    "\n",
    "    def classify(self, examples: List[Example],\n",
    "                 return_scores: bool = False) -> Union[List[int], List[float]]:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function that takes a list of examples and predicts\n",
    "        their labels using the learned classifier.\n",
    "\n",
    "        If return_scores = True, return the score prob(label = 1 | example)\n",
    "        for each example instead.\n",
    "        \"\"\"        \n",
    "        \n",
    "        label_scores = np.zeros((len(examples), 2))\n",
    "        label_scores += np.copy(self.logprior)\n",
    "        for i in range(len(examples)):\n",
    "            ex_words = remove_stop_words(examples[i].words, self.stop_words) if self.filter_stop_words else\\\n",
    "            examples[i].words\n",
    "            if self.use_bigrams:\n",
    "                ngram = self._bigram(ex_words)\n",
    "            else:\n",
    "                ngram = ex_words\n",
    "            for ng in ngram:\n",
    "                if ng in self.vocab:\n",
    "                    label_scores[i, 0] += self.logllh[(ng, 0)]\n",
    "                    label_scores[i, 1] += self.logllh[(ng, 1)]\n",
    "        label_scores = label_scores[:, 1] if return_scores else np.argmax(label_scores, axis=1)\n",
    "        return label_scores.tolist()\n",
    "\n",
    "    def get_vocab_probabilities(self, label: int) -> Dict:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function to return a dictionary of n-gram ->\n",
    "        p(label | n-gram)  for every n-gram in the vocabulary.\n",
    "\n",
    "        The n-grams will be bigrams if use_bigram = True, otherwise they\n",
    "        will be unigrams.\n",
    "        \"\"\"\n",
    "        \n",
    "        lp_post0 = {(ng, label): lllh + self.logprior[0] for (ng, label), lllh in self.logllh.items() if label == 0}\n",
    "        lp_post1 = {(ng, label): lllh + self.logprior[1] for (ng, label), lllh in self.logllh.items() if label == 1}\n",
    "        vocab_probs0 = {ng: np.exp(lpost)/(np.exp(lpost) + np.exp(lp_post1[(ng, 1)])) \\\n",
    "                        for (ng, l), lpost in lp_post0.items()}\n",
    "        vocab_probs1 = {ng: np.exp(lpost)/(np.exp(lpost) + np.exp(lp_post0[(ng, 0)])) \\\n",
    "                        for (ng, l), lpost in lp_post1.items()}\n",
    "        \n",
    "        return vocab_probs1 if label == 1 else vocab_probs0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Your job is to finish implementing the\n",
    "NaiveBayesClassifier class!\n",
    "\n",
    "The interface is very simple:\n",
    "\n",
    "* train() takes in a list of training Examples and updates the classifier based\n",
    "on the data (you'll want to save some information into the classifier class)\n",
    "* classify() takes a list of Examples (they will have labels, but\n",
    "you should not use them) and return a corresponding list of predicted labels\n",
    "(1 or 0) in the same order. If return_scores = True, it will return the score\n",
    "prob(label = 1 | example) for each example instead.\n",
    "* get_vocab_probabilities(label) should return a dictionary of n-gram ->\n",
    "p(label | n-gram)  for every n-gram in the vocabulary.\n",
    "\n",
    "Beyond that, everything else is up to you! You're free to add other helper\n",
    "methods or any other data structures/instance variables that you need.\n",
    "\n",
    "__WARNING:__ Do NOT change the interface of train() or classify(), as these\n",
    "will be called directly when grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With that said, we do have some hints and suggestions that might help along\n",
    "the way. It's totally possible to get a working solution without following\n",
    "all of these suggestions, so feel free to use or ignore them as you'd like:\n",
    "\n",
    "* We strongly recommend computing the probabilities as log\n",
    "probabilities in your implementation. Recall that your Naive Bayes prediction\n",
    "will be the argmax of a product of many probabilities (the prior probability\n",
    "$P(c)$ and a bunch of conditional probabilities $P(x | c)$ ). Each of these can\n",
    "individually be small numbers, so if you multiply many of them\n",
    "together, they may rapidly approach zero and even get rounded to 0, which will\n",
    "make it difficult or impossible for you to compare the actual values accurately.\n",
    "Instead, you can take the log of the probability, which will transform the\n",
    "product into a sum of logs. These will avoid any such bad behavior. And because\n",
    "log is a monotonically increasing function, if $log(x) > log(y)$ then $x > y$.\n",
    "So when computing your argmax, you can just compare the log probabilities\n",
    "directly and never need to worry about the true probabilities!\n",
    "* For the purposes of implementing Laplace Smoothing (+1 smoothing), it may be\n",
    "helpful to keep track of the vocabulary (the set of all words you've\n",
    "seen in the training data) or at least its' size. The Python set() class\n",
    "may be useful here.\n",
    "* There are a few different ways you can go about storing the information from\n",
    "learning in the NaiveBayesClassifier object. Ultimately, when classifying you'll\n",
    "need to compute probabilities from the counts, but it's up to you whether you'd\n",
    "like to store the raw counts or the computed probabilities.\n",
    "* You may find Python's [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) helpful\n",
    "in your implementation when counting.\n",
    "* Don't forget to implement your classifiers so they behave differently depending\n",
    "on if stop word filtering is enabled or bigrams are used.\n",
    "* You can filter stop words using the `remove_stop_words` function in `util.py`. If `filter_stop_words` is `True`, the `Classifier` will have a list of stop words stored in `self.stop_words`.\n",
    "* When using bigrams, don't forget to add the start and end markers.\n",
    "* Remember that in Python, assignment is by reference, not by value, for\n",
    "non-primitive types. Or to put things more simply, when you're assigning an\n",
    "existing list or dict to a new variable, it does NOT make a copy. It just gives\n",
    "a reference to the existing list or dict.\n",
    "\n",
    "```\n",
    "a = [1, 2, 3]\n",
    "\n",
    "# This does NOT make a copy of a. b now points to the same list as a\n",
    "b = a\n",
    "\n",
    "b.append(4)\n",
    "\n",
    "# Prints \"[1, 2, 3, 4]\"\n",
    "print(a)\n",
    "\n",
    "# If you'd like to make a copy of a list, you should do it explicitly\n",
    "b = a.copy()\n",
    "\n",
    "b.append(5)\n",
    "\n",
    "print(b) # Prints \"[1, 2, 3, 4, 5]\"\n",
    "print(a) # Prints \"[1, 2, 3, 4]\"\n",
    "```\n",
    "\n",
    "* Our reference implementation is just under 100 lines of code (including the\n",
    "skeleton code). It's quite possible that you can make a working implementation\n",
    "in fewer lines (or more lines, that's totally fine too). But if your\n",
    "implementation is way longer than this, that might be a sign that you are\n",
    "over-complicating things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once your implementation is ready, you can try evaluating it from scratch using\n",
    "this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Unigrams, no stopword removal:\n",
      "Accuracy (train): 0.82946878266654\n",
      "Accuracy (dev): 0.7329965021375826\n",
      "Performance on Unigrams w/ stopword removal:\n",
      "Accuracy (train): 0.8446735721752352\n",
      "Accuracy (dev): 0.7306645938593082\n",
      "Performance on Bigrams, no stopword removal:\n",
      "Accuracy (train): 0.44744844626057206\n",
      "Accuracy (dev): 0.4966964632724446\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance on Unigrams, no stopword removal:\")\n",
    "nb_classifier = NaiveBayesClassifier(filter_stop_words=False, use_bigrams=False)\n",
    "evaluate(nb_classifier, dataset)\n",
    "\n",
    "print(\"Performance on Unigrams w/ stopword removal:\")\n",
    "nb_classifier_swr = NaiveBayesClassifier(filter_stop_words=True, use_bigrams=False)\n",
    "evaluate(nb_classifier_swr, dataset)\n",
    "\n",
    "print(\"Performance on Bigrams, no stopword removal:\")\n",
    "nb_classifier_bigrams = NaiveBayesClassifier(filter_stop_words=False, use_bigrams=True)\n",
    "evaluate(nb_classifier_bigrams, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our implementation (using unigrams and no stopword removal) scored around 0.829\n",
    "on the training data and 0.732 on the dev data, so if you're in that ballpark\n",
    "that probably means that your implementation is working well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Once we've implemented and trained our model, it's often helpful to do some\n",
    "investigating to confirm that it's behaving the way we expect.\n",
    "\n",
    "For a Naive Bayes model, there's a couple of different ways we can do this.\n",
    "One good sanity check is to examine the learned conditional probabilities for\n",
    "each of the words in the vocabulary. Specifically, we want to find the words\n",
    "for which the probability of a particular label is high. For example, to\n",
    "find the words that the model thinks best indicate an aid-related message,\n",
    "we would want to find words with a high value of $P(\\text{label} = 1 | \\text{word})$.\n",
    "\n",
    "\n",
    "We can do this easily by calling the\n",
    "get_vocab_probabilities() method that we asked you to implement.\n",
    "\n",
    "Let's print out the top 10 highest-probability words for each label\n",
    "(positive and negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starving: prob = 0.9677079502553787\n",
      "quilts: prob = 0.9388347147222469\n",
      "clercine: prob = 0.9293606135466653\n",
      "pahore: prob = 0.9164136341755392\n",
      "rushed: prob = 0.9047786861371341\n",
      "dispatched: prob = 0.9013455936701829\n",
      "turkey: prob = 0.8976556899543069\n",
      "quarters: prob = 0.8976556899543069\n",
      "hungry: prob = 0.8900646026497829\n",
      "toothpaste: prob = 0.8796502503633159\n"
     ]
    }
   ],
   "source": [
    "vocab_probs_positive = nb_classifier.get_vocab_probabilities(1)\n",
    "top_10_positive = sorted(vocab_probs_positive.items(),\n",
    "                         key=operator.itemgetter(1), reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_positive:\n",
    "    print(\"{}: prob = {}\".format(word, prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates: prob = 0.9692000211404826\n",
      "darling: prob = 0.9663653513887357\n",
      "scholarship: prob = 0.9663653513887357\n",
      "reopening: prob = 0.9629560158916181\n",
      "reopen: prob = 0.9503824613222939\n",
      "translate: prob = 0.9503824613222939\n",
      "aur: prob = 0.9503824613222939\n",
      "updates: prob = 0.9503824613222939\n",
      "#santiago: prob = 0.9503824613222939\n",
      "translated: prob = 0.9503824613222939\n"
     ]
    }
   ],
   "source": [
    "vocab_probs_negative = nb_classifier.get_vocab_probabilities(0)\n",
    "top_10_negative = sorted(vocab_probs_negative.items(),\n",
    "                         key=operator.itemgetter(1), reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_negative:\n",
    "    print(\"{}: prob = {}\".format(word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these seem plausible to you? Do they match your expectations/agree with\n",
    "your intuition? Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another good thing to check is where your model made errors. In this case, our\n",
    "task was binary classification, so there are two possible types of errors\n",
    "that could have occurred:\n",
    "\n",
    "* Our model predicted a high probability of label = 1 for a negative example\n",
    "(false positives)\n",
    "* Our model predicted a low probability of label = 1 for a positive example\n",
    "(false negatives)\n",
    "\n",
    "We can look for exactly these two types of errors using the \"return_scores\"\n",
    "flag we asked you to implement for the classify() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "def get_false_negatives_and_false_positives(classifier, examples):\n",
    "    predicted_scores = classifier.classify(examples, return_scores=True)\n",
    "\n",
    "    false_negatives = []\n",
    "    false_positives = []\n",
    "\n",
    "    for pred_score, example in zip(\n",
    "            predicted_scores, examples):\n",
    "        if example.label == 1 and pred_score < 0.5:\n",
    "            false_negatives.append((example.words, pred_score))\n",
    "        elif example.label == 0 and pred_score >= 0.5:\n",
    "            false_positives.append((example.words, pred_score))\n",
    "\n",
    "    return false_negatives, false_positives\n",
    "\n",
    "fn, fp = get_false_negatives_and_false_positives(nb_classifier, dataset.dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have the false negatives and false positives, we can find the\n",
    "\"worst\" ones and examine them to try to figure out where our model went wrong.\n",
    "\n",
    "The \"worst\" ones would be:\n",
    "* The false negatives with the lowest probabilities of label = 1\n",
    "* The false positives with the highest probabilities of label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob = -7972.621674897531: ['we', 'used', 'to', 'harvest', '15', 'carts', 'full', 'of', 'manioc', 'a']...\n",
      "prob = -7315.880197358509: ['one', 'year', 'ago', 'when', 'a', 'catastrophic', 'earthquake', 'hit', 'china', 'canadians']...\n",
      "prob = -7010.612353402759: ['emerging', 'viral', 'diseases', 'such', 'as', 'ebola', 'marburg', 'hemorrhagic', 'fever', 'and']...\n",
      "prob = -5651.228713101871: ['since', '22', 'november', '2008', 'there', 'has', 'been', '719', '4', 'mm']...\n",
      "prob = -4838.9980756955665: ['we', 'are', 'cooped', 'up', 'all', 'day', 'here', 'tarpaulin', 'sheets', 'will']...\n",
      "prob = -4801.55169192876: ['the', 'country', 'has', 'been', 'devestated', 'by', 'two', 'decades', 'of', 'conflict']...\n",
      "prob = -3451.8256058180855: ['putih', 'river', 'and', 'pabelan', 'river', 'severe', 'overflow', 'carrying', 'mud', 'and']...\n",
      "prob = -2892.712604608727: ['diets', 'for', 'households', 'with', 'poor', 'and', 'limited', 'consumption', 'lack', 'animal']...\n",
      "prob = -2043.7900990099909: ['we', 'are', 'afraid', 'of', 'a', 'huge', 'mudslide', 'triggered', 'by', 'heavy']...\n",
      "prob = -1771.6186915813648: ['like', 'that', 'we', 'want', 'a', 'fair', 'representation', 'at', 'the', 'regional']...\n"
     ]
    }
   ],
   "source": [
    "top_10_fn = sorted(fn,\n",
    "                   key=operator.itemgetter(1))[:10]\n",
    "for words, prob in top_10_fn:\n",
    "    print(\"prob = {}: {}...\".format(prob, words[:min(len(words), 10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No false positives found!\n"
     ]
    }
   ],
   "source": [
    "if len(fp) == 0:\n",
    "    print(\"No false positives found!\")\n",
    "\n",
    "top_10_fp = sorted(fp, key=operator.itemgetter(1), reverse=True)[:10]\n",
    "for words, prob in top_10_fp:\n",
    "    print(\"prob = {}: {}\".format(prob, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do these mistakes seem reasonable/sensible? Why do you think the classifier\n",
    "misclassified them?\n",
    "\n",
    "Could some of these misclassifications be avoided if we had access to more\n",
    "training data? Or do some of them stem from the limitations of the Naive Bayes\n",
    "model itself (Hint: think about what assumptions go into the Naive Bayes model)?\n",
    "\n",
    "Some of the false negative contain words that indicates positivity, but due to the assumption of NB, other more frequent words perhaps overweigh so that the probabilities become lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hopefully these questions have gotten you thinking about what your model\n",
    "is doing and what its weaknesses and problems might be.\n",
    "\n",
    "If you have time, we encourage you to spend some more time playing around with\n",
    "your model before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, when computing your model's performances with the different settings\n",
    "(unigrams vs. bigrams, no stop word removal vs. stop word removal), you may have\n",
    "noticed something surprising.\n",
    "\n",
    "In general, we would expect models using bigrams to outperform models using\n",
    "unigrams (why?). And probably the same for stop word removal in many cases.\n",
    "\n",
    "However, you may have found that on this dataset, this does not necessarily\n",
    "occur! This does not mean that your implementation is broken or incorrect\n",
    "(although you should definitely double-check just to be sure).\n",
    "\n",
    "Why do you think this might be happening? What could be changed to get the\n",
    "expected behavior?\n",
    "\n",
    "__HINT:__ It may be helpful to consider the differences between the training\n",
    "set and dev accuracies.\n",
    "\n",
    "__HINT:__ Think back to our discussion of overfitting in the group work.\n",
    "\n",
    "Removing stop words yields a bit lower validation accuracy, and including bigram detriment the accuracy significantly for both training and validation sets. It may because the stop words include too many words than necessary for our task, and we also need more data to address the sparsity incurred by the bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A Second Attempt: Logistic Regression\n",
    "\n",
    "Now let's try tackling the same dataset with a different type of classifier:\n",
    "logistic regression.\n",
    "\n",
    "\n",
    "First, let's start off with some preliminaries to double-check our NumPy skills\n",
    "and our understanding of the logistic regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One of the building blocks of logistic regression is the sigmoid function, which\n",
    "we described in lecture. It's the method we use to convert the outputs of our\n",
    "computation $(z = w * x + b)$ from a real number between negative infinity\n",
    "and infinity to a probability between 0 and 1. Let's try implementing the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Implement the sigmoid function.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array.\n",
    "    Returns:\n",
    "        s: The numpy array with sigmoid applied element-wise\n",
    "\n",
    "    HINT: use np.exp() because your input can be a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once you're satisfied with your implementation, let's try visualizing it to\n",
    "double-check that we have the right idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, .01)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y, color='blue', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hopefully it looks reasonable! Now there's just one more building block we need,\n",
    "the logistic loss function (otherwise known as cross-entropy loss). \n",
    "\n",
    "Intuitively, it is simply a way to measure how far our prediction\n",
    "$y_{\\text{pred}} = \\sigma(WX + b)$ is from the true label $y_{\\text{true}}$.\n",
    "\n",
    "Using the formulation covered in lecture and in the notes, implement the\n",
    "logistic loss function:\n",
    "\n",
    "__NOTE:__ Your function should take in not just a single predicted label\n",
    "and true label, but a vector of predictions and a vector of true labels. This\n",
    "shouldn't affect your implementation much, as numpy allows you to operate\n",
    "naturally on vectors.\n",
    "\n",
    "__NOTE:__ Your function should return the average logistic loss over all the\n",
    "examples as a single float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    TODO: Implement the computation of the logistic loss function.\n",
    "\n",
    "    Args:\n",
    "        y_pred: A 1D vector of predicted labels for each example, of shape\n",
    "        (num_examples,).\n",
    "        y_true: A 1D vector of true labels for each example, of shape\n",
    "        (num_examples,).\n",
    "\n",
    "    Returns:\n",
    "        The average logistic loss over all examples as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    return -1*np.mean(y_true*np.log(y_pred + 1e-10) + (1 - y_true)*np.log(1 - y_pred + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To check our implementation, let's compare it to our intuition about how the\n",
    "loss should behave. It's supposed to represent how far our prediction is from\n",
    "the true label. In other words, if our prediction is way off, the loss should be\n",
    "very high. But as our prediction gets closer to the true value, it should drop\n",
    " towards 0.\n",
    "\n",
    "Let's consider the following cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted = 0, True = 1 : Loss = 23.025850929940457\n",
      "Predicted = 0.1, True = 1 : Loss = 2.302585091994046\n",
      "Predicted = 0.3, True = 1 : Loss = 1.2039728039926028\n",
      "Predicted = 0.5, True = 1 : Loss = 0.6931471803599453\n",
      "Predicted = 0.7, True = 1 : Loss = 0.3566749437958753\n",
      "Predicted = 0.9, True = 1 : Loss = 0.10536051554671516\n",
      "Predicted = 0.99, True = 1 : Loss = 0.01005033575249134\n",
      "Predicted = 0.999999, True = 1 : Loss = 9.999004999208199e-07\n",
      "Predicted = 1, True = 1 : Loss = -1.000000082690371e-10\n"
     ]
    }
   ],
   "source": [
    "def print_loss(y_pred, y_true):\n",
    "    print(\"Predicted = {}, True = {} : Loss = {}\".format(\n",
    "          y_pred, y_true, logistic_loss(np.array([y_pred]),\n",
    "                                        np.array([y_true]))))\n",
    "\n",
    "print_loss(0, 1)\n",
    "print_loss(0.1, 1)\n",
    "print_loss(0.3, 1)\n",
    "print_loss(0.5, 1)\n",
    "print_loss(0.7, 1)\n",
    "print_loss(0.9, 1)\n",
    "print_loss(0.99, 1)\n",
    "print_loss(0.999999, 1)\n",
    "print_loss(1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do the results seem reasonable? Do they agree with your intuition?\n",
    "\n",
    "What happens when y_pred = y_true? What happens when y_pred = 0 and y_true = 1?\n",
    "\n",
    "Do these seem reasonable? Do they agree with what you expect from the formula\n",
    "for the logistic loss?\n",
    "\n",
    "Why might this behavior be OK in practice, given how we are going to use the\n",
    "logistic loss (i.e. in gradient descent)?\n",
    "\n",
    "Very obviously, it works in practice and matches the intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, using the two parts you implemented above, we finally have everything we\n",
    "need to implement gradient descent, the algorithm that we'll use to learn our\n",
    "logistic regression parameters from data.\n",
    "\n",
    "We'll talk more about it next week in the context of neural networks, but for\n",
    "now we've implemented it for you, so you don't need to do anything here.\n",
    "\n",
    "However, if you're interested we encourage you to take a look and try to\n",
    "understand what the function is doing, as well as what each of the\n",
    "parameters (alpha, epsilon, num_iterations) does.\n",
    "\n",
    "You can also see how it makes use of the sigmoid and logistic loss functions\n",
    "we just implemented!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X: np.ndarray,\n",
    "                     Y: np.ndarray,\n",
    "                     batch_size: int = 2000,\n",
    "                     alpha: float = 0.5,\n",
    "                     num_iterations: int = 1000,\n",
    "                     print_every: int = 100,\n",
    "                     epsilon: float = 1e-8) -> (np.ndarray, float):\n",
    "    \"\"\"\n",
    "    Runs batch gradient descent on the provided data and returns the resulting\n",
    "    trained weight vector and bias.\n",
    "\n",
    "    Args:\n",
    "        X: A numpy array of shape (num_examples, num_features) containing\n",
    "           the training data.\n",
    "        Y: A numpy array of shape (num_examples,) containing the training\n",
    "            labels.\n",
    "        batch_size: The number of examples in each batch.\n",
    "        alpha: The learning rate for gradient descent.\n",
    "        num_iterations: The number of iterations to run gradient descent\n",
    "                        for.\n",
    "        print_every: How often (after how many iterations) to print the\n",
    "                    loss and iteration number.\n",
    "        epsilon: The early stopping condition. When the absolute change\n",
    "                 in the loss is less than epsilon, gradient descent will\n",
    "                 stop early.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, float): The learned weight vector W and bias b\n",
    "    \"\"\"\n",
    "    W = np.zeros((X.shape[1],))\n",
    "    b = 0\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    loss = 0\n",
    "    for i in range(num_iterations):\n",
    "        if batch_size >= X.shape[0]:\n",
    "            X_batch = X\n",
    "            Y_batch = Y\n",
    "        else:\n",
    "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "            X_batch = X[batch_indices, :]\n",
    "            Y_batch = Y[batch_indices]\n",
    "\n",
    "        A = sigmoid(np.dot(X_batch, W) + b)\n",
    "        dW = np.mean(np.expand_dims(A - Y_batch, axis=1) * X_batch, axis=0)\n",
    "        db = np.mean(A - Y_batch)\n",
    "        W -= alpha * dW\n",
    "        b -= alpha * db\n",
    "        prev_loss = loss\n",
    "        loss = logistic_loss(A, Y_batch)\n",
    "\n",
    "        if abs(prev_loss - loss) < epsilon:\n",
    "            break\n",
    "\n",
    "        if (i+1) % print_every == 0:\n",
    "            predictions = A\n",
    "            predictions[predictions >= 0.5] = 1\n",
    "            predictions[predictions < 0.5] = 0\n",
    "            accuracy = np.mean(predictions == Y_batch)\n",
    "            print(\"Iteration {}/{}: Batch Accuracy: {},  Batch Loss = {}\".format(\n",
    "                i + 1,\n",
    "                num_iterations,\n",
    "                accuracy,\n",
    "                loss\n",
    "            ))\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, that wraps up the preliminaries! Without any further ado,\n",
    "here's the skeleton code for the full logistic regression classifier. All you\n",
    "have to do is finish it up.\n",
    "\n",
    "It should be relatively straight-forward, given the functions you already\n",
    "implemented above. If you need a helping hand, we've provided some hints and\n",
    "suggestions below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    TODO: Implement the Logistic Regression classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 filter_stop_words: bool = None,\n",
    "                 use_bigrams: bool = False,\n",
    "                 batch_size: int = 2000,\n",
    "                 alpha: float = 0.5,\n",
    "                 num_iterations: int = 1000,\n",
    "                 print_every: int = 100,\n",
    "                 epsilon: float = 1e-8):\n",
    "        super().__init__(filter_stop_words, use_bigrams)\n",
    "\n",
    "        ngram = 2 if use_bigrams else 1\n",
    "        tokenizer = str.split if self.filter_stop_words else None\n",
    "\n",
    "        \"\"\"\n",
    "        self.vectorizer is a countVectorizer we have created for you. Use\n",
    "        it to obtain a feature vector of word counts for each example\n",
    "        in your training data.\n",
    "        Documentation: \n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "        \"\"\"\n",
    "        self.vectorizer = CountVectorizer(min_df=20,\n",
    "                                          ngram_range=(ngram, ngram),\n",
    "                                          stop_words=self.stop_words,\n",
    "                                          tokenizer=tokenizer)\n",
    "\n",
    "        # Parameters to use for gradient_descent()\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_every = print_every\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # TODO: add other data structures needed in classify() or train()\n",
    "        \n",
    "    def train(self, examples: List[Example]) -> None:\n",
    "        \"\"\"\n",
    "        TODO: Implement this!\n",
    "\n",
    "        Implement a function to train a logistic regression model.\n",
    "\n",
    "        HINT: Call gradient_descent() from above to return the learned weight\n",
    "        vector and bias. You can save these for later use in classify().\n",
    "\n",
    "        HINT: You should use the parameters (batch_size, alpha, num_iterations,\n",
    "        print_every, epsilon) provided as arguments to the\n",
    "        LogisticRegressionClassifier when calling gradient_descent().\n",
    "\n",
    "        HINT: Call self.X.toarray() after you've populated self.X with counts.\n",
    "        This converts it from a sparse matrix to a dense matrix so\n",
    "        we can use it to perform gradient descent.\n",
    "        \"\"\"\n",
    "        \n",
    "        corpus = [' '.join(ex.words) for ex in examples]\n",
    "        if self.filter_stop_words:\n",
    "            corpus = [' '.join(remove_stop_words(c.split(), self.stop_words)) for c in corpus]\n",
    "        self.X = self.vectorizer.fit_transform(corpus).toarray()\n",
    "        self.y = [ex.label for ex in examples]\n",
    "        self.W, self.b = gradient_descent(self.X, self.y,\n",
    "                             batch_size=self.batch_size,\n",
    "                             alpha=self.alpha,\n",
    "                             num_iterations=self.num_iterations,\n",
    "                             print_every=self.print_every,\n",
    "                             epsilon=self.epsilon) \n",
    "\n",
    "    def classify(self, examples: List[Example],\n",
    "                 return_scores: bool = False) -> Union[List[int], List[float]]:\n",
    "        \"\"\"\n",
    "        TODO: Implement this!\n",
    "\n",
    "        HINT: If sigmoid(X * W + b) is greater or equal to 0.5, the example\n",
    "        belongs to class 1 (positive class). Otherwise, it belongs to class 0\n",
    "        (negative class). You can use the sigmoid function you implemented\n",
    "        above.\n",
    "\n",
    "        HINT: You should use the weight vector and bias you computed\n",
    "        earlier in train().\n",
    "\n",
    "        HINT: You can use np.dot or np.matmul to do matrix multiplication.\n",
    "        \"\"\"\n",
    "        \n",
    "        corpus = [' '.join(ex.words) for ex in examples]\n",
    "        if self.filter_stop_words:\n",
    "            corpus = [' '.join(remove_stop_words(c.split(), self.stop_words)) for c in corpus]\n",
    "        self.X_test = self.vectorizer.transform(corpus).toarray()\n",
    "        y_pred = self.X_test@self.W.reshape(-1, 1) + self.b\n",
    "        if not return_scores:\n",
    "            y_pred = y_pred > 0.5\n",
    "        return y_pred.reshape(-1)\n",
    "\n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function to return the trained weights as a\n",
    "        NumPy array of shape (num_features,)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here are some tips/suggestions that may be useful to you:\n",
    "\n",
    "* If you run into \"RuntimeWarning: divide by zero encountered in log\" issues,\n",
    "try adding a very small number epsilon (like 1e-8) to the input any time\n",
    "you call np.log(). This will ensure that the input to the log is never exactly\n",
    "0, which gives an undefined result. __HINT:__ Check your logistic_loss function.\n",
    "* Depending on your implementation, your model may take some time to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're reasonably satisfied with your implementation, we can try\n",
    "evaluating it in the exact same way as you did with your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Unigrams, no stopword removal:\n",
      "Iteration 100/1000: Batch Accuracy: 0.743,  Batch Loss = 0.5366463042604861\n",
      "Iteration 200/1000: Batch Accuracy: 0.7485,  Batch Loss = 0.5156454595117926\n",
      "Iteration 300/1000: Batch Accuracy: 0.767,  Batch Loss = 0.5022727303390612\n",
      "Iteration 400/1000: Batch Accuracy: 0.7835,  Batch Loss = 0.4737976940132412\n",
      "Iteration 500/1000: Batch Accuracy: 0.7705,  Batch Loss = 0.48905021875600924\n",
      "Iteration 600/1000: Batch Accuracy: 0.7655,  Batch Loss = 0.49242783046549493\n",
      "Iteration 700/1000: Batch Accuracy: 0.794,  Batch Loss = 0.46813719897095535\n",
      "Iteration 800/1000: Batch Accuracy: 0.7685,  Batch Loss = 0.4826067416670861\n",
      "Iteration 900/1000: Batch Accuracy: 0.799,  Batch Loss = 0.4618063363762301\n",
      "Iteration 1000/1000: Batch Accuracy: 0.7955,  Batch Loss = 0.4520686199181481\n",
      "Accuracy (train): 0.7591941461560392\n",
      "Accuracy (dev): 0.739214924212981\n",
      "Performance on Unigrams w/ stopword removal:\n",
      "Iteration 100/1000: Batch Accuracy: 0.7085,  Batch Loss = 0.5793916510028253\n",
      "Iteration 200/1000: Batch Accuracy: 0.728,  Batch Loss = 0.5536868385470307\n",
      "Iteration 300/1000: Batch Accuracy: 0.732,  Batch Loss = 0.5362567584105424\n",
      "Iteration 400/1000: Batch Accuracy: 0.744,  Batch Loss = 0.5212575367279294\n",
      "Iteration 500/1000: Batch Accuracy: 0.757,  Batch Loss = 0.5096768527876747\n",
      "Iteration 600/1000: Batch Accuracy: 0.756,  Batch Loss = 0.5250739263172131\n",
      "Iteration 700/1000: Batch Accuracy: 0.7885,  Batch Loss = 0.48566503813822515\n",
      "Iteration 800/1000: Batch Accuracy: 0.762,  Batch Loss = 0.49724854750980096\n",
      "Iteration 900/1000: Batch Accuracy: 0.7725,  Batch Loss = 0.49037779256896097\n",
      "Iteration 1000/1000: Batch Accuracy: 0.7715,  Batch Loss = 0.4932531285502959\n",
      "Accuracy (train): 0.7439893566473439\n",
      "Accuracy (dev): 0.7341624562767198\n",
      "Performance on Bigrams, no stopword removal:\n",
      "Iteration 100/3000: Batch Accuracy: 0.6425,  Batch Loss = 0.6305609955471687\n",
      "Iteration 200/3000: Batch Accuracy: 0.6715,  Batch Loss = 0.6065690169332965\n",
      "Iteration 300/3000: Batch Accuracy: 0.6905,  Batch Loss = 0.5916832882818094\n",
      "Iteration 400/3000: Batch Accuracy: 0.706,  Batch Loss = 0.5789977413067348\n",
      "Iteration 500/3000: Batch Accuracy: 0.7055,  Batch Loss = 0.5765471032605024\n",
      "Iteration 600/3000: Batch Accuracy: 0.714,  Batch Loss = 0.5708679781635528\n",
      "Iteration 700/3000: Batch Accuracy: 0.7285,  Batch Loss = 0.5493857999739555\n",
      "Iteration 800/3000: Batch Accuracy: 0.717,  Batch Loss = 0.560143589794874\n",
      "Iteration 900/3000: Batch Accuracy: 0.73,  Batch Loss = 0.5506313454238204\n",
      "Iteration 1000/3000: Batch Accuracy: 0.7385,  Batch Loss = 0.5390086250157774\n",
      "Iteration 1100/3000: Batch Accuracy: 0.72,  Batch Loss = 0.5544508414301108\n",
      "Iteration 1200/3000: Batch Accuracy: 0.7405,  Batch Loss = 0.5365406934949543\n",
      "Iteration 1300/3000: Batch Accuracy: 0.71,  Batch Loss = 0.5540236597175875\n",
      "Iteration 1400/3000: Batch Accuracy: 0.737,  Batch Loss = 0.5343998006747219\n",
      "Iteration 1500/3000: Batch Accuracy: 0.7525,  Batch Loss = 0.5239390315590537\n",
      "Iteration 1600/3000: Batch Accuracy: 0.728,  Batch Loss = 0.5357401436840364\n",
      "Iteration 1700/3000: Batch Accuracy: 0.7505,  Batch Loss = 0.5290620947897264\n",
      "Iteration 1800/3000: Batch Accuracy: 0.7495,  Batch Loss = 0.5224660253760383\n",
      "Iteration 1900/3000: Batch Accuracy: 0.744,  Batch Loss = 0.5256125070361002\n",
      "Iteration 2000/3000: Batch Accuracy: 0.7515,  Batch Loss = 0.5134804089108179\n",
      "Iteration 2100/3000: Batch Accuracy: 0.747,  Batch Loss = 0.5246168603749551\n",
      "Iteration 2200/3000: Batch Accuracy: 0.763,  Batch Loss = 0.5067572642430904\n",
      "Iteration 2300/3000: Batch Accuracy: 0.7455,  Batch Loss = 0.5187961896479758\n",
      "Iteration 2400/3000: Batch Accuracy: 0.766,  Batch Loss = 0.5071614652271778\n",
      "Iteration 2500/3000: Batch Accuracy: 0.7525,  Batch Loss = 0.5137421421069901\n",
      "Iteration 2600/3000: Batch Accuracy: 0.758,  Batch Loss = 0.5056261703124473\n",
      "Iteration 2700/3000: Batch Accuracy: 0.7565,  Batch Loss = 0.518928098248727\n",
      "Iteration 2800/3000: Batch Accuracy: 0.7585,  Batch Loss = 0.5123959248799633\n",
      "Iteration 2900/3000: Batch Accuracy: 0.7535,  Batch Loss = 0.5112243123402466\n",
      "Iteration 3000/3000: Batch Accuracy: 0.7655,  Batch Loss = 0.5045929624183711\n",
      "Accuracy (train): 0.7257436092369096\n",
      "Accuracy (dev): 0.7065682083171395\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance on Unigrams, no stopword removal:\")\n",
    "lr_classifier = LogisticRegressionClassifier(filter_stop_words=False, use_bigrams=False)\n",
    "evaluate(lr_classifier, dataset)\n",
    "\n",
    "print(\"Performance on Unigrams w/ stopword removal:\")\n",
    "lr_classifier_swr = LogisticRegressionClassifier(filter_stop_words=True, use_bigrams=False)\n",
    "evaluate(lr_classifier_swr, dataset)\n",
    "\n",
    "# Note that we run for more iterations here\n",
    "print(\"Performance on Bigrams, no stopword removal:\")\n",
    "lr_classifier_bigrams = LogisticRegressionClassifier(filter_stop_words=False, use_bigrams=True, num_iterations=3000)\n",
    "evaluate(lr_classifier_bigrams, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation (using unigrams with no stop word removal) scored around\n",
    "0.791 on the training data and 0.770 on the dev data, so if you're in that\n",
    "ballpark that probably means that your implementation is working well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Again, now that we have our trained model, we can try to examine it to better\n",
    "understand what it is doing.\n",
    "\n",
    "If we think back to our Naive Bayes model, after training we have access to\n",
    "the conditional probabilities for each word (n-gram) given each label. We\n",
    "were able to examine them to get an idea of what words our model associates\n",
    "with each label.\n",
    "\n",
    "For our logistic regression model, after training we have access to a weight\n",
    "vector that contains a weight for each feature. We can connect these weights\n",
    "back to the list of features (in our case, these will be unigrams or bigrams,\n",
    "depending on which one we used for our model).\n",
    "\n",
    "Features with larger weights are those that the model associates with the\n",
    "positive label, and those with smaller weights are those the model associates\n",
    "with the negative label.\n",
    "\n",
    "If it's not clear why this is true, try thinking back to the equation used\n",
    "to compute the logistic regression output. If a weight for a feature is a large\n",
    "positive number, and that feature appears in an example, what will happen\n",
    " to the output for that example? What about a feature with a large negative\n",
    " weight?\n",
    "\n",
    "Let's examine the features with the largest and smallest weights in our\n",
    "trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "features = lr_classifier.vectorizer.get_feature_names()\n",
    "weights = lr_classifier.get_weights()\n",
    "\n",
    "features_to_weights = [(features[i], weights[i])\n",
    "                    for i in range(len(features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food: weight = 1.1946533339506993\n",
      "tents: weight = 1.039397167233884\n",
      "aid: weight = 1.0053755050335105\n",
      "tent: weight = 0.982196362927209\n",
      "shelter: weight = 0.9707143227360808\n",
      "victims: weight = 0.9476657931338486\n",
      "help: weight = 0.8438074805903363\n",
      "relief: weight = 0.8339062113834802\n",
      "earthquake: weight = 0.7969560831239864\n",
      "dead: weight = 0.7890258415721039\n"
     ]
    }
   ],
   "source": [
    "top_10_features = sorted(features_to_weights,\n",
    "                   key=operator.itemgetter(1), reverse=True)[:10]\n",
    "for feature, weight in top_10_features:\n",
    "    print(\"{}: weight = {}\".format(feature, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job: weight = -0.9445856054517553\n",
      "santiago: weight = -0.5818107746540966\n",
      "notes: weight = -0.4729893476366554\n",
      "message: weight = -0.4038016529907111\n",
      "development: weight = -0.3945061753924365\n",
      "level: weight = -0.3801484016431029\n",
      "agriculture: weight = -0.36429377544246055\n",
      "information: weight = -0.3553427655380138\n",
      "4636: weight = -0.3508147901105934\n",
      "work: weight = -0.3467085650629597\n"
     ]
    }
   ],
   "source": [
    "bottom_10_features = sorted(features_to_weights,\n",
    "                            key=operator.itemgetter(1))[:10]\n",
    "for feature, weight in bottom_10_features:\n",
    "    print(\"{}: weight = {}\".format(feature, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do these features make sense to you? Do they agree with your intuition? Are\n",
    "they similar to or different from the features/words you saw associated with\n",
    "each class by the Naive Bayes model?\n",
    "\n",
    "The words are different compared to NB, but equally reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also do the same thing that we did with the Naive Bayes model by\n",
    "identifying the \"worst\" errors our model makes on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "fn, fp = get_false_negatives_and_false_positives(lr_classifier, dataset.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob = -2.822401905492472: [\"i'd\", 'like', 'to', 'get', 'more', 'information', 'about', 'the', 'possibility', 'of']...\n",
      "prob = -2.3688373779834184: ['hi', '4636', 'did', 'you', 'give', 'the', 'news', 'for', 'tonight', 'on']...\n",
      "prob = -2.3082014715323615: ['raising', 'the', 'alert', 'status', 'to', 'level', '4', 'the', 'highest', 'level']...\n",
      "prob = -2.257273469228468: ['however', 'during', 'the', 'third', 'week', 'of', 'october', 'perhaps', 'in', 'reaction']...\n",
      "prob = -2.23590905297752: ['downtown', 'santiago', 'security', 'scares', 'spreading', 'but', 'is', 'it', 'all', 'in']...\n",
      "prob = -2.1772888091760625: ['i', 'would', 'like', 'information', 'on', 'evacuation', 'of', 'haitians', 'who', 'would']...\n",
      "prob = -2.177111490448154: ['ctfu', 'ahurricanesandy', 'dis', 'bitch', 'waz', 'walking', 'her', 'dog', 'and', 'i']...\n",
      "prob = -2.0453121845342883: ['livestock', 'crops', 'rural', 'and', 'urban', 'housing', 'roads', 'river', 'and', 'canal']...\n",
      "prob = -1.9642533330147707: ['we', 'will', 'ensure', 'that', 'adequate', 'quantities', 'of', 'foodgrains', 'and', 'other']...\n",
      "prob = -1.955025833297348: ['it', 'is', 'cold', 'in', 'cuba', 'this', 'morning', 'it', 'could', 'reach']...\n"
     ]
    }
   ],
   "source": [
    "top_10_fn = sorted(fn,\n",
    "                   key=operator.itemgetter(1))[:10]\n",
    "for words, prob in top_10_fn:\n",
    "    print(\"prob = {}: {}...\".format(prob, words[:min(len(words), 10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob = 5.930379914027449: ['these', 'communities', 'are', 'in', 'areas', 'experiencing', 'chronic', 'food', 'insecurity', 'environmental']...\n",
      "prob = 4.066591536072641: ['if', 'households', 'had', 'no', 'recourse', 'to', 'subsidized', 'maize', 'and', 'had']...\n",
      "prob = 3.7663922629506663: ['i', 'am', 'a', 'licensed', 'teacher', 'i', 'am', 'licensed', 'in', 'cpr']...\n",
      "prob = 3.2345473348171607: ['we', 'need', 'help', 'at', 'place', 'mausolee', 'facing', 'the', 'palais', 'de']...\n",
      "prob = 3.2243314324171024: ['the', 'company', 'i', 'work', 'for', 'regeneron', 'pharmaceuticals', 'has', 'a', 'team']...\n",
      "prob = 3.0115422973425305: ['mostly', 'young', 'people', 'they', 'spend', 'their', 'days', 'collecting', 'trash', 'spraying']...\n",
      "prob = 2.7728316049969646: ['even', 'as', 'oxfam', 'america', 'and', 'many', 'other', 'relief', 'and', 'development']...\n",
      "prob = 2.736625726280253: ['my', 'house', 'collapsed', 'i', 'need', 'food', 'and', 'water', 'i', \"don't\"]...\n",
      "prob = 2.6279827188228637: ['how', 'we', 'can', 'find', 'food', 'and', 'water', 'we', 'have', 'people']...\n",
      "prob = 2.5361105370570742: ['the', 'statement', 'clarified', 'that', \"'myanmar\", 'is', 'not', 'yet', 'ready', 'to']...\n"
     ]
    }
   ],
   "source": [
    "if len(fp) == 0:\n",
    "    print(\"No false positives found!\")\n",
    "\n",
    "top_10_fp = sorted(fp, key=operator.itemgetter(1), reverse=True)[:10]\n",
    "for words, prob in top_10_fp:\n",
    "    print(\"prob = {}: {}...\".format(prob, words[:min(len(words), 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do these errors seem reasonable? Can you think of why the model may have made\n",
    "them? Are they similar to or different from the errors you saw from the Naive\n",
    "Bayes model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Further questions to consider:\n",
    "\n",
    "* Did logistic regression outperform Naive Bayes? Did you expect it to? Why or\n",
    "why not?\n",
    "* How do the different settings (unigrams vs. bigrams, stop word removal vs.\n",
    "no stop word removal) affect the performance relative to each other? Relative\n",
    "to Naive Bayes? Do these results seem reasonable/expected? If not, what might\n",
    "explain what you're seeing?\n",
    "\n",
    "Logistic regression performs better on the validation set. When it comes to stop words and bigrams, similar argument from NB holds for LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation Redux\n",
    "\n",
    "__NOTE: Your implementation will not be evaluated on its performance\n",
    "on this part!__\n",
    "\n",
    "Now that we've verified that our methods behave correctly on a given\n",
    "dataset, it is easy to apply them and evaluate on an alternative dataset and\n",
    "see if they perform differently.\n",
    "\n",
    "In `data/coronavirus`, we have provided a second dataset consisting of reddit\n",
    "comments on posts related to the COVID-19 pandemic early last year. Each\n",
    "example is a single line in the csv, consisting of a comment and associated\n",
    "sentiment label (0 for negative, 1 for positive). It consists of around 4 times\n",
    " as many examples as the triage/disaster dataset.\n",
    "\n",
    "The task on this dataset is to, given the text of a comment, predict its\n",
    "sentiment label. This is an example of sentiment analysis (specifically\n",
    "sentiment classification), another type of NLP task.\n",
    "\n",
    "In this particular case, you could imagine using sentiment analysis tools\n",
    "to get an approximate idea of the mood social media (i.e. Reddit) users feel\n",
    "about the pandemic at a particular point in time. If we succeeded in training\n",
    "a good classifier to identify positive and negative sentiment comments/posts,\n",
    "we could use it to count the number of positive and negative posts each day,\n",
    "giving an approximate measure of social media sentiment.\n",
    "\n",
    "This information could be very useful for governments or NGOs trying to gauge\n",
    "the public response to COVID-19 policies, or identify how the pandemic is\n",
    "affecting mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Although the task is different, we can straightforwardly load and examine\n",
    "the new data the same way we did with the previous dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'util.Dataset'>\n",
      "dataset.train contains 80000 examples\n",
      "First training example:\n",
      "Words: ['Thats', 'probably', 'best', 'left', 'to', 'a', 'second', 'post', 'These', 'are', 'stats', 'youre', 'talking', 'about', 'analysis']\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "covid_dataset = load_data(\"./data/coronavirus\")\n",
    "print(type(covid_dataset))\n",
    "\n",
    "print(\"dataset.train contains {} examples\".format(len(covid_dataset.train)))\n",
    "\n",
    "print(\"First training example:\")\n",
    "print(\"Words: {}\".format(covid_dataset.train[0].words))\n",
    "print(\"Label: {}\".format(covid_dataset.train[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we can also train our models in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (train): 0.8691\n",
      "Accuracy (dev): 0.7853\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = NaiveBayesClassifier(filter_stop_words=False, use_bigrams=False)\n",
    "\n",
    "evaluate(nb_classifier, covid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/1000: Batch Accuracy: 0.716,  Batch Loss = 0.5848350601407173\n",
      "Iteration 200/1000: Batch Accuracy: 0.7395,  Batch Loss = 0.5489141460520967\n",
      "Iteration 300/1000: Batch Accuracy: 0.7795,  Batch Loss = 0.5235115139626257\n",
      "Iteration 400/1000: Batch Accuracy: 0.801,  Batch Loss = 0.49519759606472546\n",
      "Iteration 500/1000: Batch Accuracy: 0.782,  Batch Loss = 0.49711390123755084\n",
      "Iteration 600/1000: Batch Accuracy: 0.8115,  Batch Loss = 0.47644750901118627\n",
      "Iteration 700/1000: Batch Accuracy: 0.8175,  Batch Loss = 0.45696205542462964\n",
      "Iteration 800/1000: Batch Accuracy: 0.8125,  Batch Loss = 0.4569664572667337\n",
      "Iteration 900/1000: Batch Accuracy: 0.831,  Batch Loss = 0.44467430625060705\n",
      "Iteration 1000/1000: Batch Accuracy: 0.814,  Batch Loss = 0.45655503620344356\n",
      "Accuracy (train): 0.7944\n",
      "Accuracy (dev): 0.7669\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegressionClassifier(filter_stop_words=False,\n",
    "                                             use_bigrams=False)\n",
    "\n",
    "evaluate(lr_classifier, covid_dataset, limit_training_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What do you notice about the results you got on this dataset compared to the\n",
    "previous one?\n",
    "\n",
    "What about the relative performances of the two models? Has that changed? If so,\n",
    "why do you think that might be?\n",
    "\n",
    "Finally, for the logistic regression model, you can try adjusting the values\n",
    "of some of the arguments to LogisticRegressionClassifier, like alpha or\n",
    "num_iterations, re-running the evaluation, and seeing if the performance has\n",
    "changed. We haven't really discussed what these values represent, but you\n",
    "should be able to notice that by adjusting them you can significantly change\n",
    "your model's performance!\n",
    "\n",
    "These are examples of __hyperparameters__ of our model. You can\n",
    " think of them as adjustable settings that control how our model works and\n",
    " how it learns. Oftentimes you will want to experiment with different choices\n",
    " for these parameters to find ones that work best and give the best possible\n",
    " performance. This optimal choice of the hyperparameters\n",
    "will depend on the particular dataset/task as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found notebook file, creating submission zip...\n",
      "  adding: pa2.ipynb (deflated 66%)\n",
      "  adding: deps/ (stored 0%)\n",
      "  adding: deps/example_dep.txt (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa2.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa2.ipynb deps/\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa2.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa2.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA2 Triage assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
